{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_wwm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eOhvTIPydMW"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "dataset_directory_path = './'\n",
        "saved_weights_path = './bert_wwm_saved_weights.pt'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0-08wnFRDHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d15d88-bdf1-4265-9895-975b60db75ce"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a18dtyDtRWfp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ja99Q93toE-"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXczEbwUmjjA"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt5LItI3Raqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6ab898ea-af9b-4ccc-d681-c3dec1a71219"
      },
      "source": [
        "training_set = pd.read_csv(\"chinese_train.csv\").dropna().reset_index(drop=True)\n",
        "training_texts = training_set['text']\n",
        "training_labels = training_set['label']\n",
        "\n",
        "testing_set = pd.read_csv(\"chinese_test.csv\").dropna().reset_index(drop=True)\n",
        "testing_texts = testing_set['text']\n",
        "testing_labels = testing_set['label']\n",
        "\n",
        "validation_set = pd.read_csv(\"chinese_validate.csv\").dropna().reset_index(drop=True)\n",
        "validation_texts = validation_set['text']\n",
        "validation_labels = validation_set['label']\n",
        "\n",
        "training_set.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>回复新浪网友对博文国家文物局限制鉴宝节目现场估价转的评论查看原文</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>分享网易新闻发生在昆明的火锅店老板辱滇门云南人该愤怒还是羞愧发生在昆明网易新闻客户端网易新闻</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>西宁城管围殴民警扬言要把警察打死西宁城管围</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>川航航班因驾驶舱风挡破裂安全备降成都今天上午626从重庆江北国际机场出发前往拉萨的四川航空3...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>分享网易新闻湖南道县一按摩店员工因琐事矛盾捅死老板夫妻</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0                   回复新浪网友对博文国家文物局限制鉴宝节目现场估价转的评论查看原文      1\n",
              "1     分享网易新闻发生在昆明的火锅店老板辱滇门云南人该愤怒还是羞愧发生在昆明网易新闻客户端网易新闻      1\n",
              "2                              西宁城管围殴民警扬言要把警察打死西宁城管围      1\n",
              "3  川航航班因驾驶舱风挡破裂安全备降成都今天上午626从重庆江北国际机场出发前往拉萨的四川航空3...      1\n",
              "4                        分享网易新闻湖南道县一按摩店员工因琐事矛盾捅死老板夫妻      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMOPs-wbR58f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5388c80a-38e6-4a74-fd90-152f605acbf8"
      },
      "source": [
        "training_labels.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    10375\n",
              "0    10304\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA-UDnYoalgR",
        "outputId": "cb98e764-55a9-4634-966d-2eeaa5c4771c"
      },
      "source": [
        "training_texts"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         回复新浪网友对博文国家文物局限制鉴宝节目现场估价转的评论查看原文\n",
              "1           分享网易新闻发生在昆明的火锅店老板辱滇门云南人该愤怒还是羞愧发生在昆明网易新闻客户端网易新闻\n",
              "2                                    西宁城管围殴民警扬言要把警察打死西宁城管围\n",
              "3        川航航班因驾驶舱风挡破裂安全备降成都今天上午626从重庆江北国际机场出发前往拉萨的四川航空3...\n",
              "4                              分享网易新闻湖南道县一按摩店员工因琐事矛盾捅死老板夫妻\n",
              "                               ...                        \n",
              "20674    塞尔维亚总统求助中国据俄罗斯媒体报道称近期日本委托驻日美军运输机调运我国捐赠日方的2000个...\n",
              "20675    重庆加州花园第二次起火市民再次掀翻私家车为消防让路6月19日网曝重庆加州花园发生大火私家车阻...\n",
              "20676    凌晨6点开始宜昌城市主干道和高速路封路开始封城了一小时后襄阳封城至此湖北封省湖北加油兄弟姐妹...\n",
              "20677    美国的10艘医疗军舰已经开进纽约港每艘船上有1000张病床上面拥有所有的医疗抢救设施每艘船相...\n",
              "20678    美国的10艘医疗军舰已经开进纽约港每艘船上有1000张病床上面拥有所有的医疗抢救设施每艘船相...\n",
              "Name: text, Length: 20679, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q1z7y13UkIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "76817750-0348-4a19-b02e-6d2e066f4d74"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in training_texts]\n",
        "\n",
        "pd.Series(seq_len).hist(range=(0,200))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f405c793990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhQJDQfRSGHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4027a6d5-7279-4a6f-cf30-f136b6a11635"
      },
      "source": [
        "# For BERT\n",
        "from transformers import BertTokenizerFast as TokenizerClass\n",
        "from transformers import BertForSequenceClassification as ModelClass\n",
        "model_name = 'hfl/chinese-bert-wwm'\n",
        "tokenizer = TokenizerClass.from_pretrained(model_name)\n",
        "# pt_model = ModelClass.from_pretrained(model_name)\n",
        "pt_model = ModelClass.from_pretrained(model_name, num_labels=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6CTWIhYciFF"
      },
      "source": [
        "training_encodings = tokenizer(training_texts.tolist(), max_length=25, padding='max_length', truncation=True)\n",
        "validation_encodings = tokenizer(validation_texts.tolist(), max_length=25, padding='max_length', truncation=True)\n",
        "testing_encodings = tokenizer(testing_texts.tolist(), max_length=25, padding='max_length', truncation=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJb0D-VcVph"
      },
      "source": [
        "class CovidDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "training_dataset = CovidDataset(training_encodings, training_labels)\n",
        "validation_dataset = CovidDataset(validation_encodings, validation_labels)\n",
        "testing_dataset = CovidDataset(testing_encodings, testing_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbmU0hGOdoDR"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "testing_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sbDkm29ZZbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451881a1-a861-439b-e5a1-306c4282fc70"
      },
      "source": [
        "pt_model.to(device)\n",
        "pt_model.train()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cGFNjEDZlMh"
      },
      "source": [
        "# optimizer\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(pt_model.parameters(), lr = 1e-5) # learning rate"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AAayVrtwPTV"
      },
      "source": [
        "def train(model, dataloader):\n",
        "  print('Training...')\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  t0 = time.time()\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = pt_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs[0]\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  return avg_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN7EWn2Ww6xJ"
      },
      "source": [
        "def evaluate(model, dataloader):\n",
        "  print('Evaluating...')\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  t0 = time.time()\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = pt_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs[0]\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  return avg_loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6Rv3TvaAKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54ca780-b529-4e5b-8366-da9f66c31ba6"
      },
      "source": [
        "training_losses, validation_losses = [], []\n",
        "best_validation_loss = float('inf')\n",
        "for epoch in range(epochs):\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "  training_loss = train(pt_model, training_loader)\n",
        "  validation_loss = evaluate(pt_model, validation_loader)\n",
        "  if validation_loss < best_validation_loss:\n",
        "    best_validation_loss = validation_loss\n",
        "    torch.save(pt_model.state_dict(), saved_weights_path)\n",
        "  training_losses.append(training_loss)\n",
        "  validation_losses.append(validation_loss)\n",
        "  print(f'\\nAverage training loss: {training_loss:.3f}')\n",
        "  print(f'Average validation loss: {validation_loss:.3f}\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:08.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:53.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:19.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.269\n",
            "Average validation loss: 0.146\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:43.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:09.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:44.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.099\n",
            "Average validation loss: 0.036\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.047\n",
            "Average validation loss: 0.015\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:43.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:44.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.029\n",
            "Average validation loss: 0.009\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.023\n",
            "Average validation loss: 0.007\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:43.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.014\n",
            "Average validation loss: 0.006\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.013\n",
            "Average validation loss: 0.005\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:19.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.011\n",
            "Average validation loss: 0.002\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:44.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.011\n",
            "Average validation loss: 0.004\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    50  of    647.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    647.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    647.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    647.    Elapsed: 0:00:35.\n",
            "  Batch   250  of    647.    Elapsed: 0:00:44.\n",
            "  Batch   300  of    647.    Elapsed: 0:00:52.\n",
            "  Batch   350  of    647.    Elapsed: 0:01:01.\n",
            "  Batch   400  of    647.    Elapsed: 0:01:10.\n",
            "  Batch   450  of    647.    Elapsed: 0:01:18.\n",
            "  Batch   500  of    647.    Elapsed: 0:01:27.\n",
            "  Batch   550  of    647.    Elapsed: 0:01:36.\n",
            "  Batch   600  of    647.    Elapsed: 0:01:45.\n",
            "Evaluating...\n",
            "  Batch    50  of    216.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    216.    Elapsed: 0:00:17.\n",
            "  Batch   150  of    216.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    216.    Elapsed: 0:00:35.\n",
            "\n",
            "Average training loss: 0.007\n",
            "Average validation loss: 0.004\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPxjmAAyuPGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "e5c7ff06-adf9-452b-d6f0-50272223f5ea"
      },
      "source": [
        "import plotly.express as px\n",
        "f = pd.DataFrame(training_losses)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Training Loss of The Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"0d22dc25-eeea-457d-a3cd-ffa137750a93\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"0d22dc25-eeea-457d-a3cd-ffa137750a93\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '0d22dc25-eeea-457d-a3cd-ffa137750a93',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"index=%{x}<br>Loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"xaxis\": \"x\", \"y\": [0.2689263325017608, 0.09895269977507488, 0.04678826380867966, 0.0289897465040379, 0.022844249287656535, 0.01420372937429625, 0.013080110558325484, 0.010775066686001027, 0.011069890297142998, 0.007178748991780375], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training Loss of The Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0d22dc25-eeea-457d-a3cd-ffa137750a93');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULM5IITX5Ulr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "ee1868a8-c46d-4afd-954b-1da6d8178fd2"
      },
      "source": [
        "f = pd.DataFrame(validation_losses)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Validation Loss of The Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"31e5bb58-4e9f-46e5-9b19-f68a26734346\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"31e5bb58-4e9f-46e5-9b19-f68a26734346\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '31e5bb58-4e9f-46e5-9b19-f68a26734346',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"index=%{x}<br>Loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"xaxis\": \"x\", \"y\": [0.145715490595817, 0.03569358287495561, 0.014688043632136046, 0.009230244001661992, 0.007325596822839644, 0.006187611750748247, 0.004806870999851122, 0.0021212798978585516, 0.00395200331163155, 0.004032350695937297], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Validation Loss of The Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('31e5bb58-4e9f-46e5-9b19-f68a26734346');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ba3Dkz20yVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09da1d95-47bd-4476-c03d-a839b644814d"
      },
      "source": [
        "#load weights of best model\n",
        "pt_model.load_state_dict(torch.load(saved_weights_path))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbWukUlhe9WH"
      },
      "source": [
        "# get predictions for test data\n",
        "\n",
        "with torch.no_grad():\n",
        "  # empty list to save the model predictions\n",
        "  total_preds, true_labels = [], []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step, batch in enumerate(testing_loader):\n",
        "    # push the batch to gpu\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = pt_model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    total_preds.append(logits)\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIgv1nKz6RSF"
      },
      "source": [
        "total_preds = np.concatenate(total_preds, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketTrmv-6YYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa546a82-6509-43de-b9a2-9564af5d192b"
      },
      "source": [
        "len(total_preds)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6897"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK3xMXVi6r0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75028d37-b97b-40e8-f7bf-0117749dae90"
      },
      "source": [
        "len(true_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6897"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvCwbAggfDho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c01731f-b389-489a-9f74-9d9784c06289"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# model's performance\n",
        "preds = np.argmax(total_preds, axis = 1)\n",
        "print(classification_report(true_labels, preds))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96      3436\n",
            "           1       0.96      0.96      0.96      3461\n",
            "\n",
            "    accuracy                           0.96      6897\n",
            "   macro avg       0.96      0.96      0.96      6897\n",
            "weighted avg       0.96      0.96      0.96      6897\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs8w-DNofFZ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "6494e0ae-10ce-46b9-b78b-86a3eb9631ec"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(true_labels, preds)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3312</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>124</td>\n",
              "      <td>3337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1\n",
              "row_0            \n",
              "0      3312   124\n",
              "1       124  3337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv5cwjkm67OD"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}